{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b59c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV\n",
    "from sklearn.metrics import (\n",
    " r2_score, mean_absolute_error, mean_squared_error,\n",
    ")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "from scipy import stats\n",
    "from equipy.fairness import FairWasserstein\n",
    "from equipy.metrics import unfairness\n",
    "from models_and_metrics import *\n",
    "from real_datasets import *\n",
    "from scipy.stats import kstest, ks_2samp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c22aad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_df_labeled = \"gossis_labeled.csv\"\n",
    "chemin_df_unlabeled = \"gossis_unlabeled.csv\"\n",
    "df_labeled = pd.read_csv(chemin_df_labeled)\n",
    "df_unlabeled = pd.read_csv(chemin_df_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "353bc4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1 – INITIAL LOAD & CLEANING\n",
      "============================================================\n",
      "\n",
      "Initial shape: Labeled=(91713, 180), Unlabeled=(39308, 180)\n",
      "\n",
      "Filtering columns (≤10 % NA) – keeping 81 features.\n",
      "→ Shape after NA cleaning: Labeled=(56451, 81), Unlabeled=(23100, 80)\n",
      "\n",
      "Encoding sensitive group 'African American' in column 'ethnicity' …\n",
      "→ Unique values: [-1  1]\n",
      "\n",
      "Extracting pre‑computed scores from 'apache_4a_hospital_death_prob' …\n"
     ]
    }
   ],
   "source": [
    "labeled_data, blackbox_scores_labeled, s_labeled, unlabeled_data, blackbox_scores_unlabeled, s_unlabeled = load_Gossis_data.load_gossis_data(\n",
    "    labeled=df_labeled,\n",
    "    unlabeled=df_unlabeled,\n",
    "    target_feature=\"hospital_death\",\n",
    "    sensitive_group=\"African American\",\n",
    "    blackbox_feature=\"apache_4a_hospital_death_prob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f66bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_variable_types(df, columns=None, discrete_threshold=10, unique_ratio_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Identifies whether variables are continuous or discrete.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame\n",
    "    - columns: List of columns to check (None = all columns)\n",
    "    - discrete_threshold: Maximum number of unique values to consider a variable as discrete\n",
    "    - unique_ratio_threshold: Minimum ratio of unique values/total to consider a variable as continuous\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary with results and a DataFrame summarizing the characteristics\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    \n",
    "    results = {\n",
    "        'continuous': [],\n",
    "        'discrete': [],\n",
    "        'categorical': [],\n",
    "        'binary': [],\n",
    "        'summary': []\n",
    "    }\n",
    "    \n",
    "    for col in columns:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            results['categorical'].append(col)\n",
    "            continue\n",
    "            \n",
    "        n_unique = df[col].nunique()\n",
    "        n_total = len(df[col])\n",
    "        unique_ratio = n_unique / n_total\n",
    "        \n",
    "        # Collect statistics\n",
    "        summary = {\n",
    "            'column': col,\n",
    "            'dtype': df[col].dtype,\n",
    "            'n_unique': n_unique,\n",
    "            'unique_ratio': unique_ratio,\n",
    "            'min': df[col].min(),\n",
    "            'max': df[col].max(),\n",
    "            'has_decimals': any(x % 1 != 0 for x in df[col].dropna().sample(min(1000, len(df[col]))).values)\n",
    "        }\n",
    "        results['summary'].append(summary)\n",
    "        \n",
    "        # Classify the variable\n",
    "        if n_unique == 2:\n",
    "            results['binary'].append(col)\n",
    "            results['discrete'].append(col)\n",
    "        elif n_unique <= discrete_threshold or unique_ratio < unique_ratio_threshold:\n",
    "            results['discrete'].append(col)\n",
    "        else:\n",
    "            # Check if the variable has decimal values\n",
    "            if summary['has_decimals']:\n",
    "                results['continuous'].append(col)\n",
    "            else:\n",
    "                # If many unique values but all integers, it's probably an ID or a discrete variable\n",
    "                if n_unique > 100:\n",
    "                    results['continuous'].append(col)  # Probably a numeric ID\n",
    "                else:\n",
    "                    results['discrete'].append(col)\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    summary_df = pd.DataFrame(results['summary'])\n",
    "    if not summary_df.empty:\n",
    "        summary_df['classification'] = summary_df['column'].apply(\n",
    "            lambda x: 'binary' if x in results['binary'] \n",
    "                     else 'discrete' if x in results['discrete'] \n",
    "                     else 'continuous' if x in results['continuous']\n",
    "                     else 'categorical'\n",
    "        )\n",
    "    \n",
    "    return results, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00185d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_uncorrelated_features(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Select features with correlation coefficients below the input threshold.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the features to analyze\n",
    "        threshold: threshold of maximal accepted correlation\n",
    "        \n",
    "    Returns:\n",
    "        List of variables to keep\n",
    "    \"\"\"\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Identify columns to delete\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    # Columns to keep\n",
    "    to_keep = [column for column in df.columns if column not in to_drop]\n",
    "    \n",
    "    return to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed8cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_continuous_candidates = labeled_data.describe().columns.to_list()\n",
    "results, summary_df = identify_variable_types(labeled_data, columns=list_continuous_candidates)\n",
    "list_columns_to_test = summary_df[summary_df['n_unique']>70]['column'].to_list()\n",
    "numerical_features_to_test = [i for i in list_columns_to_test if i not in [\"apache_2_bodysystem\", \"icu_admit_source\",'encounter_id','patient_id', 'hospital_id']]\n",
    "list_less_correlated_var = select_uncorrelated_features(labeled_data[numerical_features_to_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of selected features\n",
    "categorical_features = [\"apache_2_bodysystem\", \"icu_admit_source\"]\n",
    "ordinal_features = [\"gender\"]\n",
    "numerical_features = list_less_correlated_var\n",
    "# [\n",
    "#     \"age\", \"bmi\", \"immunosuppression\",\n",
    "#     \"d1_diasbp_noninvasive_max\", \"d1_sysbp_noninvasive_max\",\n",
    "#     \"d1_heartrate_min\", \"resprate_apache\", \"d1_sysbp_min\",\n",
    "#     \"d1_spo2_min\", \"d1_glucose_max\", \"pre_icu_los_days\", \"map_apache\",\"apache_4a_hospital_death_prob\"\n",
    "# ]\n",
    "sensitive_feature=\"ethnicity\"\n",
    "target_feature = \"hospital_death\"\n",
    "\n",
    "all_selected_features = categorical_features + ordinal_features + numerical_features + [sensitive_feature, target_feature]\n",
    "df_analysis = labeled_data[all_selected_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4b395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2 – FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "Original feature sample:\n",
      "  apache_2_bodysystem      icu_admit_source gender   age    bmi  height  \\\n",
      "0      Cardiovascular                 Floor      M  68.0  22.73   180.3   \n",
      "1         Respiratory                 Floor      F  77.0  27.42   160.0   \n",
      "5          Neurologic  Accident & Emergency      M  67.0  27.56   190.5   \n",
      "\n",
      "   pre_icu_los_days  apache_3j_diagnosis  heart_rate_apache  map_apache  ...  \\\n",
      "0          0.541667               502.01              118.0        40.0  ...   \n",
      "1          0.927778               203.01              120.0        46.0  ...   \n",
      "5          0.000694               403.01              113.0       130.0  ...   \n",
      "\n",
      "   h1_spo2_min  h1_sysbp_max  d1_creatinine_max  d1_glucose_max  \\\n",
      "0         74.0         131.0               2.51           168.0   \n",
      "1         70.0          95.0               0.71           145.0   \n",
      "5         97.0         143.0               0.71           156.0   \n",
      "\n",
      "   d1_glucose_min  d1_potassium_max  d1_potassium_min  \\\n",
      "0           109.0               4.0               3.4   \n",
      "1           128.0               4.2               3.8   \n",
      "5           125.0               3.9               3.7   \n",
      "\n",
      "   apache_4a_hospital_death_prob  ethnicity  hospital_death  \n",
      "0                           0.10         -1               0  \n",
      "1                           0.47         -1               0  \n",
      "5                           0.05         -1               0  \n",
      "\n",
      "[3 rows x 34 columns]\n",
      "Shape: (56451, 34)\n",
      "\n",
      "Concatenating labeled + unlabeled for consistent encoding …\n",
      "\n",
      "Label‑encoding 1 ordinal variable(s) …\n",
      "\n",
      "One‑Hot‑encoding 2 categorical variable(s) …\n",
      "→ Shape (79551, 33) → (79551, 46)\n",
      "\n",
      "Encoded feature sample:\n",
      "   gender   age    bmi  height  pre_icu_los_days  apache_3j_diagnosis  \\\n",
      "0       1  68.0  22.73   180.3          0.541667               502.01   \n",
      "1       0  77.0  27.42   160.0          0.927778               203.01   \n",
      "2       1  67.0  27.56   190.5          0.000694               403.01   \n",
      "\n",
      "   heart_rate_apache  map_apache  resprate_apache  temp_apache  ...  \\\n",
      "0              118.0        40.0             36.0         39.3  ...   \n",
      "1              120.0        46.0             33.0         35.1  ...   \n",
      "2              113.0       130.0             35.0         36.6  ...   \n",
      "\n",
      "   apache_2_bodysystem_Respiratory  apache_2_bodysystem_Trauma  \\\n",
      "0                            False                       False   \n",
      "1                             True                       False   \n",
      "2                            False                       False   \n",
      "\n",
      "   apache_2_bodysystem_Undefined Diagnoses  \\\n",
      "0                                    False   \n",
      "1                                    False   \n",
      "2                                    False   \n",
      "\n",
      "   apache_2_bodysystem_Undefined diagnoses  \\\n",
      "0                                    False   \n",
      "1                                    False   \n",
      "2                                    False   \n",
      "\n",
      "   icu_admit_source_Accident & Emergency  icu_admit_source_Floor  \\\n",
      "0                                  False                    True   \n",
      "1                                  False                    True   \n",
      "2                                   True                   False   \n",
      "\n",
      "   icu_admit_source_Operating Room / Recovery  \\\n",
      "0                                       False   \n",
      "1                                       False   \n",
      "2                                       False   \n",
      "\n",
      "   icu_admit_source_Other Hospital  icu_admit_source_Other ICU  hospital_death  \n",
      "0                            False                       False               0  \n",
      "1                            False                       False               0  \n",
      "2                            False                       False               0  \n",
      "\n",
      "[3 rows x 47 columns]\n",
      "\n",
      "NOTE – feature scaling left to downstream pipeline to prevent leakage.\n"
     ]
    }
   ],
   "source": [
    "labeled_data_preprocessed, unlabeled_data_preprocessed, data_orig_names = load_Gossis_data.preprocess_features(\n",
    "    labeled_data,\n",
    "    unlabeled_data,\n",
    "    target_feature,\n",
    "    sensitive_feature,\n",
    "    categorical_features,\n",
    "    ordinal_features,\n",
    "    numerical_features, # Renommé de no_change_features\n",
    "    do_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bad895e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ethnicity\n",
       "1    50345\n",
       "2     6106\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data_preprocessed.replace({'ethnicity': {1: 2}},inplace=True)\n",
    "labeled_data_preprocessed.replace({'ethnicity': {-1: 1}},inplace=True)\n",
    "unlabeled_data_preprocessed.replace({'ethnicity': {1: 2}},inplace=True)\n",
    "unlabeled_data_preprocessed.replace({'ethnicity': {-1: 1}},inplace=True)\n",
    "labeled_data_preprocessed['ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be903d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_predictions = ['y_input_reg', 'y_pred_fair', 'y_score_equipy', 'y_pred_riken']\n",
    "list_description = ['Regression Lineaire Input','Modèle Lineaire Fair','Equipy','Riken']\n",
    "S_variable='ethnicity'\n",
    "train_dataset, test_dataset = train_test_split(labeled_data_preprocessed, test_size=0.2)\n",
    "train_dataset,pool_dataset = train_test_split(train_dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa510e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_columns = unlabeled_data_preprocessed.select_dtypes(include='bool').columns.tolist()\n",
    "unlabeled_data_preprocessed[bool_columns] = unlabeled_data_preprocessed[bool_columns].astype(int)\n",
    "bool_columns = labeled_data_preprocessed.select_dtypes(include='bool').columns.tolist()\n",
    "labeled_data_preprocessed[bool_columns] = labeled_data_preprocessed[bool_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16a79de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= 'h1_diasbp_max'\n",
    "S_variable ='ethnicity'\n",
    "X_features=labeled_data_preprocessed.drop(columns=[S_variable,'h1_diasbp_max','hospital_death']).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(data_name, y, S_variable, X_features, data, n_simulations):\n",
    "    \"\"\"\n",
    "    Runs an experiment by varying a specific parameter.\n",
    "    \n",
    "    Args:\n",
    "        data_name: Name of the dataset\n",
    "        y: Target variable\n",
    "        S_variable: Sensitive attribute\n",
    "        X_features: List of feature variables\n",
    "        data: Dataset\n",
    "        n_simulations: Number of bootstrap simulations to run\n",
    "    \n",
    "    Returns:\n",
    "        all_results: Dictionary containing results for each parameter value\n",
    "    \"\"\"\n",
    "    # List of models and metrics\n",
    "    models = ['y_pred_fair', 'y_input_reg', 'y_score_equipy', 'y_pred_riken', 'y_pred_bias']\n",
    "    metrics = ['r2', 'GRW2','mae', 'rmse', 'unfairness_W2', 'unfairness_W1', 'ks_stat','beta_0_NoStd','beta_NoStd','gamma_NoStd',\n",
    "           'beta_0_1_Std','beta_0_2_Std','beta_1_Std','beta_2_Std','gamma_Std',\n",
    "            'fair_intercept_1_NoStd', 'fair_intercept_2_NoStd', 'beta_1_NoStd','beta_2_NoStd', \n",
    "            'fair_intercept_Std','beta_Std',\n",
    "            'riken_intercept_Std']\n",
    "    param_riken_null = 0\n",
    "\n",
    "    # Dictionary to store all results\n",
    "    all_results = {}\n",
    "\n",
    "    # Initialize results dictionary for each model and metric\n",
    "    t_results = {model: {metric: [] for metric in metrics} for model in models}\n",
    "        \n",
    "    # Perform n_simulations for this parameter value\n",
    "    for bootstrap in tqdm(range(n_simulations)):\n",
    "\n",
    "        # Data preparation\n",
    "        train_dataset, test_dataset = train_test_split(data, test_size=0.2, random_state=bootstrap)\n",
    "        train_dataset, pool_dataset = train_test_split(train_dataset, test_size=0.2, random_state=bootstrap)\n",
    "        \n",
    "        for s in data[S_variable].unique():\n",
    "            # Check if there's enough data for each group\n",
    "            if len(train_dataset[train_dataset[S_variable]==s]//3) < 12*len(X_features):\n",
    "                print(f'normalized_beta_{s}__and__norm_beta_{s}__are_set_to_0')\n",
    "                param_riken_null += 1\n",
    "            if len(train_dataset[train_dataset[S_variable]==s]//2) < 18*len(X_features):\n",
    "                print(f'beta_{s}_bis__is_set_to_0')\n",
    "                param_riken_null += 1\n",
    "\n",
    "        y_sensitive_feature = pd.DataFrame({f\"{S_variable}\": test_dataset[S_variable].to_list()})\n",
    "        unique_groups = test_dataset[S_variable].unique()\n",
    "            \n",
    "        # Fair Linear Model\n",
    "        coef_input_model, param_dictionnary, input_model, test_dataset = Fair_model.predict_fair_linear_score(\n",
    "                train_dataset, pool_dataset, test_dataset, S_variable, y, X_features, True, False, False\n",
    "            )\n",
    "\n",
    "        # EquiPy Model\n",
    "        Benchmark_model.benchmark_equipy(train_dataset, test_dataset, 'y_input_reg', S_variable)\n",
    "            \n",
    "        # Riken Model\n",
    "        dictionnary_riken_raw = Benchmark_model.riken_prediction(train_dataset, test_dataset, S_variable, X_features, y)\n",
    "            \n",
    "        # Evgeni Model (model_bias)\n",
    "        Benchmark_model.weighted_group_intercepts(train_dataset, test_dataset, X_features, y, S_variable, True)\n",
    "            \n",
    "        # Calculate and store metrics for each model\n",
    "        for prediction in models:\n",
    "            t_results[prediction]['r2'].append(r2_score(test_dataset[y], test_dataset[prediction]))\n",
    "            t_results[prediction]['GRW2'].append(Metrics.group_weighted_r2(test_dataset, y, prediction, S_variable))\n",
    "            t_results[prediction]['rmse'].append(np.sqrt(mean_squared_error(test_dataset[y], test_dataset[prediction])))\n",
    "            t_results[prediction]['unfairness_W1'].append(unfairness(np.array(test_dataset[prediction].tolist()), y_sensitive_feature))\n",
    "            t_results[prediction]['unfairness_W2'].append(Metrics.unfairness_computation(prediction, S_variable, test_dataset))\n",
    "            \n",
    "            t_results['y_input_reg']['beta_0_NoStd'].append(param_dictionnary['beta_0'])\n",
    "            t_results['y_input_reg']['beta_NoStd'].append(param_dictionnary['beta'])\n",
    "            t_results['y_input_reg']['gamma_NoStd'].append(param_dictionnary['gamma'])\n",
    "            t_results['y_input_reg']['beta_0_1_Std'].append(param_dictionnary['beta_0']+np.dot(param_dictionnary['empirical_mean_1'],param_dictionnary['beta']))\n",
    "            t_results['y_input_reg']['beta_0_2_Std'].append(param_dictionnary['beta_0']+np.dot(param_dictionnary['empirical_mean_2'],param_dictionnary['beta']))\n",
    "            t_results['y_input_reg']['beta_1_Std'].append(param_dictionnary['beta']*param_dictionnary['var_cov_product_1'])\n",
    "            t_results['y_input_reg']['beta_2_Std'].append(param_dictionnary['beta']*param_dictionnary['var_cov_product_2'])\n",
    "            t_results['y_input_reg']['gamma_Std'].append(param_dictionnary['gamma'])\n",
    "\n",
    "            t_results['y_pred_fair']['fair_intercept_1_NoStd'].append(param_dictionnary['fair_intercept']-np.dot(param_dictionnary['empirical_mean_1'],param_dictionnary['beta'])/param_dictionnary['var_cov_product_1'])\n",
    "            t_results['y_pred_fair']['fair_intercept_2_NoStd'].append(param_dictionnary['fair_intercept']-np.dot(param_dictionnary['empirical_mean_2'],param_dictionnary['beta'])/param_dictionnary['var_cov_product_2'])\n",
    "            t_results['y_pred_fair']['beta_1_NoStd'].append(param_dictionnary['invariant_var_cov_term']*param_dictionnary['beta']/param_dictionnary['var_cov_product_1'])\n",
    "            t_results['y_pred_fair']['beta_2_NoStd'].append(param_dictionnary['invariant_var_cov_term']*param_dictionnary['beta']/param_dictionnary['var_cov_product_2'])\n",
    "            t_results['y_pred_fair']['gamma_NoStd'].append(0)\n",
    "            t_results['y_pred_fair']['fair_intercept_Std'].append(param_dictionnary['fair_intercept'])\n",
    "            t_results['y_pred_fair']['beta_Std'].append(param_dictionnary['invariant_var_cov_term'])\n",
    "            t_results['y_pred_fair']['gamma_Std'].append(0)\n",
    "\n",
    "            if len(unique_groups) >= 2:\n",
    "                # Calculate Kolmogorov-Smirnov statistic between groups\n",
    "                ks_stat = kstest(\n",
    "                    rvs=test_dataset[test_dataset[S_variable] == unique_groups[0]][prediction],\n",
    "                    cdf=test_dataset[test_dataset[S_variable] == unique_groups[1]][prediction],\n",
    "                    alternative='two-sided'\n",
    "                ).statistic\n",
    "                t_results[prediction]['ks_stat'].append(ks_stat)\n",
    "        \n",
    "    # Calculate means and standard deviations for each metric\n",
    "    summary_t = {}\n",
    "    for model in models:\n",
    "        summary_t[model] = {}\n",
    "        for metric in metrics:\n",
    "            if t_results[model][metric]:  # Check if the list is not empty\n",
    "                summary_t[model][f'{metric}_mean'] = round(np.mean(t_results[model][metric]), 5)\n",
    "                summary_t[model][f'{metric}_std'] = round(np.std(t_results[model][metric]), 5)\n",
    "            else:\n",
    "                summary_t[model][f'{metric}_mean'] = None\n",
    "                summary_t[model][f'{metric}_std'] = None\n",
    "        \n",
    "    return summary_t, t_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1c534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [10:11<14:20, 30.72s/it]"
     ]
    }
   ],
   "source": [
    "summary_t,t_results= run_experiment('GOSSIS_GRW2',y, S_variable, X_features, labeled_data_preprocessed, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
